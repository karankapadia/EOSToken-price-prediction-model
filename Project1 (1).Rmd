---
title: "Project Report"
author: 'Group Members: Sagar Patel(sbp170003) Karan Kapadia(kxk170017) Esha Punjabi(EHP170000)'
output:
  html_document: default
  pdf_document: default
---

#Introduction

##Ethereum:

<font size="4">The Ethereum project was created in July 2015 based on the proposal by Vitalik Buterin. Like Bitcoin, Ethereum is a distributed public blockchain network. In the Ethereum blockchain, instead of mining for bitcoin, miners work to earn Ether, a type of crypto token that fuels the network. Beyond a tradeable cryptocurrency, Ether is also used by application developers to pay for transaction fees and services on the Ethereum network. Ethereum blockchain has some extraordinary capabilities. One of them is that you can build smart contracts. It's kind of what it sounds like. It's a contract that self-executes, and the contract handles the enforcement, the management, performance, and payment</font>


##ERC20:

<font size="4">ERC20 is a protocol standard that defines certain rules and standards for issuing tokens on Ethereum's network. In 'ERC20', ERC stands for Ethereum Request For Comments and 20 stands for a unique ID number to distinguish this standard from others. To put it in layman terms, if you include certain functions in the token's smart contract, you are ERC20 compliant. If you don't include the mandatory functions, you are not ERC20. ERC 20 tokens don't have their dedicated blockchain and thrive on Ethereum's blockchain instead. This is the reason why, when you send ERC20 tokens, you are required to have some Ethereum as GAS.</font>


##About Primary Token:

<font size="4">In this project, we selected EOS token as our primary token. EOS tokens are ERC-20 compatible tokens distributed on the Ethereum blockchain pursuant to a related ERC-20 smart contract (the "EOS Tokens"). Recently, EOS has been so successful that it is on the verge of becoming its own blockchain technology like ethereum. We are going to mine the EOS token data using statistical methods. EOS token has 18 decimals so it has total of 10^18 sub units. The no of transactions the data has is 2326723. The Token consists of datapoints in the following format : fromNodeID toNodeID unixTime tokenAmount.</font>



##Project Question1:
   
<font size="4">In this, we will be using EOS token to model the plotting distribution of how many times a user buys, sells a token. Find the best distribution fit by estimating the distribution parameters.</font>
   
## 1.Selection of Token:
###Token Name:

### 2.Loading the Data
```{r}
eos_token <- read.table("~/Desktop/R/Ethereum token graphs/networkeosTX.txt", quote="\"", comment.char="", header = FALSE)
colnames(eos_token) <- c("fromnodeID", "tonodeId", "unixtime", "tokenamount")
eos_frame<-data.frame(eos_token)
head(eos_frame)
```

### 3.Data Preprocessing-Removing Outliers

<font size="4">In Data Preprocessing, we need to filter out all the data that has tokenAmount that is greater then total amount of Token. So, Total amount of Token = total supply * number of decimals. For eos token in etheruem, it has total supply of 10^9 and subunits of 18 decimal.

Total amount of eos_token=(10^9*10^18).</font>

### 3.1 find outliers

```{r}
outlierdata<-eos_frame[which (eos_frame$tokenamount > (10^27)),]
```

Total Number of Outliers:

```{r}
message("Total number of outliers:",length(outlierdata$tokenamount))
```

### 3.2 Number Of users involved in outlier Transactions

```{r}
users<-c(outlierdata$fromnodeID,outlierdata$tonodeId)
uniqueusers<-unique(users)
message("Total number Of users involved in outlier Transactions:",length(uniqueusers))
```

### 3.3 Remove outliers

```{r}
datawithoutoutlier<-eos_frame[which (eos_frame$tokenamount < (10^27)),]
```

Data without Outliers:

```{r}
message("Amount of data without outliers:",length(datawithoutoutlier$tokenamount))
```

## 4. Find the Distribution of Buyers
### 4.1 Find the frequency of buyers

```{r}
frequency_table<-table(datawithoutoutlier["tonodeId"])
freq<-as.data.frame(frequency_table)
colnames(freq)<-c("BuyerId","Frequency")
head(freq)
```

### 4.2 Find the frequency of number of buys of a token 

```{r}
frequency_buyers<-table(freq$Frequency)
freq<-as.data.frame(frequency_table)
colnames(freq)<-c("NoOfBuys","FrequencyofNoOfBuys")
head(freq)
```

### 4.3 Plot the bar plot for frequency of number of buys of a token(x=buys,y=frequency)

```{r}
barplot(freq$FrequencyofNoOfBuys,names.arg = freq$NoOfBuys,ylab = "frequencyNoBuys", xlab = "NoBuys", xlim = c(0,70),ylim = c(0,140000))
```

### 4.4 Fit Distribution. For this, we are using 'fitdistrplus' library.
Loading required package: MASS,survival,npsurv,lsei

```{r}
library(fitdistrplus)
message("mean of frequency of number of buys is ", mean(freq$FrequencyofNoOfBuys))
fit <- fitdist(freq$FrequencyofNoOfBuys, "pois", method="mle")
fit
plot(fit)
summary(fit)
```

## 5. Find the Distribution of Sellers  
### 5.1 Find the frequency of Sellers

```{r}
frequency_table<-table(datawithoutoutlier["fromnodeID"])
freq1<-as.data.frame(frequency_table)
colnames(freq1)<-c("SellerId","Frequency")
```

### 5.2 Find the frequency of number of buys of a token

```{r}
frequency_Sellers<-table(freq1$Frequency)
freq1<-as.data.frame(frequency_table)
colnames(freq1)<-c("NoOfSells","FrequencyofNoOfSells")
```

### 5.3 Plot the bar plot for frequency of number of sells of a token(x=sells,y=frequency)

```{r}
barplot(freq1$FrequencyofNoOfSells,names.arg = freq1$NoOfSells,ylab = "frequencyNoSells", xlab = "NoofSells", xlim = c(0,20),ylim = c(0,140000))
```

### 5.4 Fit Distribution. For this, we are using 'fitdistrplus' library.

Loading required package: MASS,survival,npsurv,lsei
Using library fitdistrplus
```{r}
library(fitdistrplus)
message("mean of frequency of number of sells is ", mean(freq1$FrequencyofNoOfSells))
fit2 <- fitdist(freq1$FrequencyofNoOfSells, "pois", method="mle")
fit2
plot(fit2)
summary(fit2)
```

# Conclusion:- 

<font size="4">From the computations that we have done, We can conclude that mean and lambda are giving the same value, so our assumption about poission distribution seems correct.</font>

# Project Question2:

<font size="4">Here, we created layers of transaction using the bin selection in histogram. After creating layers, we computed a feature in every layer. We selected number of transactions as feature. Then we computed the number of transaction happened at a particular day for a particular layer. Also, we calculated the correlation between the feature and the token prices for every layer.</font>

### Creating layers using bin selection of histogram
```{r}
#Finding out the values of interquartile range.
summary(datawithoutoutlier$tokenamount) 
#Histogram of samples from third quartile to max amount
layer1 <- datawithoutoutlier[which (datawithoutoutlier$tokenamount>=1.093e+21),]
#Histogram of samples between first to third quartile
remaining_layers <- datawithoutoutlier[which (datawithoutoutlier$tokenamount<1.093e+21 & datawithoutoutlier$tokenamount>=4.100e+19 ),] 
#histogram of sample between min to first quartile.
layer5 <- datawithoutoutlier[which (datawithoutoutlier$tokenamount<4.100e+19 & datawithoutoutlier$tokenamount>=1.000e+00 ),] 
#hist(remaining_layers$tokenamount, breaks = "Sturges")
hist(remaining_layers$tokenamount,  xlab="token amount", border = "blue", col ="grey", breaks = 3)
summary(remaining_layers$tokenamount)
breakpoints = c(4.100e+19, 2.151e+20, 4.993e+20, 1.093e+21)
hist(remaining_layers$tokenamount, breaks = breakpoints)
# Now their are 5 layers in total.
```


```{r}
#Now making unequal bin size 3 layers
layer2 <- remaining_layers[which (remaining_layers$tokenamount>=4.100e+19 & remaining_layers$tokenamount<2.151e+20 ),]
layer3 <- remaining_layers[which (remaining_layers$tokenamount>=2.151e+20 & remaining_layers$tokenamount<4.993e+20 ),]
layer4 <- remaining_layers[which (remaining_layers$tokenamount>=4.993e+20 & remaining_layers$tokenamount<1.093e+21 ),]
```


### Trial and error of finding layers using Doane's method

```{r}
#Tried making histogram with the doane's formula but eventually didn't work as the number of bins are more than expected and so its difficult to extract correlation from the layers.
skew <- function(x) {
  xbar <- mean(x)
  m3 <- mean((x-xbar)^3)
  m2 <- mean((x-xbar)^2)
  return (m3/m2^1.5)
}
n <- 2313151
x <- datawithoutoutlier$tokenamount
absb1 = abs(skew(x))
se = sqrt( 6 * (n - 2)/((n + 1) * (n + 3)) )
Ke = log2( 1 + absb1/se )
nclass.Doane = ceiling( nclass.Sturges(x) + Ke ) 
print(nclass.Doane) 
R = diff(range(x)) 
h.Doane = R/nclass.Doane; print(h.Doane) 
breaks.Doane = min(x) + h.Doane*seq(0,nclass.Doane)
print(round(breaks.Doane)) 
```

### Converted unixtime with date and time format for every layer.

```{r}
#For layer1
val1 <- layer1$unixtime 
layer1$unixtime <- as.POSIXct(val1, origin="1970-01-01")
layer1$unixtime = as.Date(layer1$unixtime, format = "%Y/%m/%d %I:%M:%S") 
layer1$unixtime = as.Date(format(layer1$unixtime, "%Y-%m-%d"))
```

```{r}
# Converted unixtime with date and time format.
#For layer2
val2 <- layer2$unixtime 
layer2$unixtime <- as.POSIXct(val2, origin="1970-01-01")
layer2$unixtime = as.Date(layer2$unixtime, format = "%Y/%m/%d %I:%M:%S") 
layer2$unixtime = as.Date(format(layer2$unixtime, "%Y-%m-%d"))
```

```{r}
# Converted unixtime with date and time format.
#For layer3
val3 <- layer3$unixtime 
layer3$unixtime <- as.POSIXct(val3, origin="1970-01-01")
layer3$unixtime = as.Date(layer3$unixtime, format = "%Y/%m/%d %I:%M:%S") 
layer3$unixtime = as.Date(format(layer3$unixtime, "%Y-%m-%d"))
```

```{r}
# Converted unixtime with date and time format.
#For layer4
val4 <- layer4$unixtime 
layer4$unixtime <- as.POSIXct(val4, origin="1970-01-01")
layer4$unixtime = as.Date(layer4$unixtime, format = "%Y/%m/%d %I:%M:%S") 
layer4$unixtime = as.Date(format(layer4$unixtime, "%Y-%m-%d"))
```

```{r}
# Converted unixtime with date and time format.
#For layer5
val5 <- layer5$unixtime 
layer5$unixtime <- as.POSIXct(val5, origin="1970-01-01")
layer5$unixtime = as.Date(layer5$unixtime, format = "%Y/%m/%d %I:%M:%S") 
layer5$unixtime = as.Date(format(layer5$unixtime, "%Y-%m-%d"))
```

### Finding out the number of transaction performed on "2018-04-24" in layer1 similarly we can find the no of transaction occured on particular date for every layer
```{r}
my.data.frame<-layer2[which (layer2$unixtime == "2018-04-24"),]
nrow(my.data.frame)
```

```{r}

eos_price<- read.delim("~/Desktop/R/eos_price.txt")
eos_price$Date = format(as.Date(eos_price$Date, '%m/%d/%y'), "%m/%d/%Y")

```

### Creating a new data frame for every layer which has columns of Date, Closing price of token, frequency i.e. no of transaction occured on that particular date.

```{r}
# For layer1
df_2 = table(layer1["unixtime"])
final_layer1 <- as.data.frame(df_2)
colnames(final_layer1) <- c("date", "frequency")
final_layer1$date = format(as.Date(final_layer1$date, '%Y-%m-%d'), "%m/%d/%Y")
final_layer1["close_price"] <- NA

for(i in seq(1,nrow(final_layer1),1))
{
  for(j in seq(1, nrow(eos_price),1))
  {
    if(final_layer1[i, "date"] == eos_price[j, "Date"]){
      final_layer1[i, "close_price"] <- eos_price[j, "Close"]
    }
  }
}
final_layer1 <- na.omit(final_layer1)
```

```{r}
# For layer2
df_2 = table(layer2["unixtime"])
final_layer2 <- as.data.frame(df_2)
colnames(final_layer2) <- c("date", "frequency")
final_layer2$date = format(as.Date(final_layer2$date, '%Y-%m-%d'), "%m/%d/%Y")
final_layer2["close_price"] <- NA

for(i in seq(1,nrow(final_layer2),1))
{
  for(j in seq(1, nrow(eos_price),1))
  {
    if(final_layer2[i, "date"] == eos_price[j, "Date"]){
      final_layer2[i, "close_price"] <- eos_price[j, "Close"]
    }
  }
}
final_layer2 <- na.omit(final_layer2)
```

```{r}
# For layer3
df_2 = table(layer3["unixtime"])
final_layer3 <- as.data.frame(df_2)
colnames(final_layer3) <- c("date", "frequency")
final_layer3$date = format(as.Date(final_layer3$date, '%Y-%m-%d'), "%m/%d/%Y")
final_layer3["close_price"] <- NA

for(i in seq(1,nrow(final_layer3),1))
{
  for(j in seq(1, nrow(eos_price),1))
  {
    if(final_layer3[i, "date"] == eos_price[j, "Date"]){
      final_layer3[i, "close_price"] <- eos_price[j, "Close"]
    }
  }
}
final_layer3 <- na.omit(final_layer3)
```

```{r}
# For layer4
df_2 = table(layer4["unixtime"])
final_layer4 <- as.data.frame(df_2)
colnames(final_layer4) <- c("date", "frequency")
final_layer4$date = format(as.Date(final_layer4$date, '%Y-%m-%d'), "%m/%d/%Y")
final_layer4["close_price"] <- NA

for(i in seq(1,nrow(final_layer4),1))
{
  for(j in seq(1, nrow(eos_price),1))
  {
    if(final_layer4[i, "date"] == eos_price[j, "Date"]){
      final_layer4[i, "close_price"] <- eos_price[j, "Close"]
    }
  }
}
final_layer4 <- na.omit(final_layer4)
```

```{r}
# For layer5
df_2 = table(layer5["unixtime"])
final_layer5 <- as.data.frame(df_2)
colnames(final_layer5) <- c("date", "frequency")
final_layer5$date = format(as.Date(final_layer5$date, '%Y-%m-%d'), "%m/%d/%Y")
final_layer5["close_price"] <- NA

for(i in seq(1,nrow(final_layer5),1))
{
  for(j in seq(1, nrow(eos_price),1))
  {
    if(final_layer5[i, "date"] == eos_price[j, "Date"]){
      final_layer5[i, "close_price"] <- eos_price[j, "Close"]
    }
  }
}
final_layer5 <- na.omit(final_layer5)
```

### Now Finding the correlation of Price vs Feature. We are using cor.test() function for this.

```{r}
# Here, we have only shown correlation with respect of layer 5. Similarly, we can get the correlation for every layer.
cor.test(final_layer5$close_price, final_layer5$frequency, method = "pearson", use = "complete.obs")
#cor.test(final_layer1$close_price, final_layer5$frequency, method = "kendall", use = "complete.obs")
```

### Using library 'ggpubr', we ploted the scatterplot of the correlation of layer 5.
```{r}
library("ggpubr")
ggscatter(final_layer5, x = "frequency", y = "close_price", add = "reg.line", conf.int = TRUE, cor.coef = TRUE,cor.method = "pearson", xlab = "Price", ylab = "Feature(No of Transaction)")
```

# Conclusion

<font size="4">From the output value, we see that the features taken for correlation are highly correlated. As the number of transactions in the data is too huge after removing the outliers and the distribution is too right skewed, it is necessary to make layers of transaction with appropriate bin size to obtain perfect correlation. According to the result, we can conclude that using 5 layers that are created above generates almost good correlation. Layer 5 has the highest correlation of 0.66 among all the layers.</font>

# Project Question3:

<font size="4">Finding the most active buyers and sellers in the  primary token network, and tracking them in other tokens. Fit a distribution for the number of unique tokens they invest in. For this question, use networks of all tokens, and see if primary token's buyers/sellers appear in other tokens.</font>

### Loading the Data
```{r}
eos_token <- read.table("~/Desktop/R/Ethereum token graphs/networkeosTX.txt", quote="\"", comment.char="", header = FALSE)
colnames(eos_token) <- c("fromnodeID", "tonodeId", "unixtime", "tokenamount")
eos_frame<-data.frame(eos_token)
head(eos_frame)
```

### Data Preprocessing-Removing Outliers

<font size="4">In Data Preprocessing, we need to filter out all the data that has tokenAmount that is greater then total amount of Token.So, Total amount of Token = total supply * number of decimals. For eos token in etheruem, it has total supply of 10^9 and subunits of 18 decimal.

Total amount of eos_token=(10^9*10^18).</font>

### find outliers

```{r}
outlierdata<-eos_frame[which (eos_frame$tokenamount > (10^27)),]
```
Total Number of Outliers:
```{r}
message("Total number of outliers:",length(outlierdata$tokenamount))
```

### Remove outliers

```{r}
datawithoutoutlier<-eos_frame[which (eos_frame$tokenamount < (10^27)),]
```

### Now fetching just the details about the buyer's id and their frequency of transaction and storing them in a data frame. 

```{r}
frequency_table<-table(datawithoutoutlier["tonodeId"])
freq<-as.data.frame(frequency_table)
colnames(freq)<-c("BuyerId","Frequency")
head(freq)

active_buyers<-freq[rev(order(freq$Frequency)), ]
```

### Now selecting the top 30 active buyers.

```{r}
top_active_buyers<-head(active_buyers,30)
```

### Calculating the count of each buyer that involves in the transcation of unique token which is in the top buyer's list and printing that count as a vector of 30 top buyers. For this, we are using the network of all tokens by setting the working directory as the path where all the token files are located.

```{r}
setwd("~/Desktop/R/Ethereum token graphs");
fileNames <- Sys.glob("*.txt")
files<-list.files();

count <- vector();
increment=1;
for (bid in top_active_buyers$BuyerId) {
  counter = 0;
  
  for(f in files){
  df<-read.delim(f,sep=" ",header = FALSE);
  if(bid %in% df[,2]){
      counter = counter +1;
      }
  }
  count[increment]=counter;
  increment=increment+1;
}

count;
```

### Plotting a linear graph of top_buyers VS No_of_tokens.

```{r}
x<-c(1:30);
y<-count;
plot(x,y,type = "p",xlab = "Top 30 Active Buyers",ylab = "No_of_tokens",main = "Buyers investing in unique tokens")
```

### Now fitting the distribution for the count of buyers indulged in unique tokens data.For this, we are using 'fitdistrplus' library.

```{r}
library(fitdistrplus)
fit <- fitdist(count, "pois", method="mle")
plot(fit)
summary(fit)
summary(count)
```


### Now fetching just the details about the seller's id and their freuency of transaction and storing them in a data frame. 

```{r}
frequency_table1<-table(datawithoutoutlier["fromnodeID"])
freq1<-as.data.frame(frequency_table1)
colnames(freq1)<-c("SellerId","Frequency")
head(freq1)

active_sellers<-freq1[rev(order(freq1$Frequency)), ]
```
### Now selecting the top 30 active sellers.

```{r}
top_active_sellers<-head(active_sellers,30)
```

### Calculating the count of each seller that involves in the transcation of unique token which is in the top seller's list and printing that count as a vector of 30 top sellers. For this, we are using the network of all tokens by setting the working directory as the path where all the token files are located.

```{r}
setwd("~/Desktop/R/Ethereum token graphs");
fileNames <- Sys.glob("*.txt")
files<-list.files();

count1 <- vector();
increment=1;
for (sid in top_active_sellers$SellerId) {
  counter = 0;
  
  for(f in files){
  df<-read.delim(f,sep=" ",header = FALSE);
  if(sid %in% df[,1]){
      counter = counter +1;
      }
  }
  count1[increment]=counter;
  increment=increment+1;
}

count1
```

### Plotting a linear graph of top_buyers VS No_of_tokens.

```{r}
x<-c(1:30);
y<-count1;
plot(x,y,type = "p",xlab = "Top 30 Active Sellers",ylab = "No_of_tokens",main = "Sellers trading unique tokens")
```

### Now fitting the distribution for the count of sellers indulged in unique tokens data. For this, we are using 'fitdistrplus' library.

```{r}
library(fitdistrplus)
fit2 <- fitdist(count1, "pois", method="mle")
plot(fit2)
summary(fit2)
summary(count1)
```

# Conclusion:

<font size = "4">From the mean of the data of both the counts obtained and the lambda value, we can deduct that the data follows the poisson distribution.</font>


# PROJECT 2 

<font size="4"> Create a Multiple Linear Regression model to explain the price return on day t using the features of day t-1. </font> 

### Loading the Data

```{r}
eos_token <- read.table("~/Desktop/R/Ethereum token graphs/networkeosTX.txt", quote="\"", comment.char="", header = FALSE)
colnames(eos_token) <- c("fromnodeID", "tonodeId", "unixtime", "tokenamount")
eos_frame<-data.frame(eos_token)
head(eos_frame)
```

### Remove outliers

```{r}
datawithoutoutlier<-eos_frame[which (eos_frame$tokenamount < (10^27)),]
```

### Here we are changing the format of the date in the token-price file.

```{r}
eos_price<- read.delim("~/Desktop/R/eos_price.txt")
eos_price$Date = format(as.Date(eos_price$Date, '%m/%d/%y'), "%m/%d/%Y")

```

#### Here we are converting unixtime to date format in the token file and changing it to the format so that it is compatible to the date of token-price file .

```{r}
datawithoutoutlier$unixtime <- as.POSIXct(datawithoutoutlier$unixtime, origin="1970-01-01")
datawithoutoutlier$unixtime = as.Date(datawithoutoutlier$unixtime, format = "%Y/%m/%d %I:%M:%S")
datawithoutoutlier$unixtime = as.Date(format(datawithoutoutlier$unixtime, "%Y-%m-%d"))
datawithoutoutlier$unixtime = format(as.Date(datawithoutoutlier$unixtime,'%Y-m-d'), "%m/%d/%Y")
```

### We are creating a new data frame that contains the features that will be used in creating the linear model. We are calculating the number of transaction occuring on the particular day and creating a new data frame for that.

```{r}

feature_df <- eos_price[,c(1,2,3,4,5)]
feature_df$no_of_transaction <- NULL

freq_date <- table(datawithoutoutlier$unixtime)
freq_table <- as.data.frame(freq_date)
freq_table$Date <- as.Date(format(freq_table$Date,'%Y-%m-%d'),"%m/%d/%Y")
colnames(freq_table) <- c("Date","No_Of_Transaction")

```

### Then these two data frames are merged on the same column - 'Date' and assigning names to the columns to the newly created dataframe.

```{r}
allfeatures_df<-merge(feature_df,freq_table,"Date")
colnames(allfeatures_df) <- c("Date","Open","High","Low","Close","No_Of_Transaction")
```

### Price-Return is calculated for each day which will be the 'y' value in the model. Price-Return is calculated as the difference between the closing price of two consecutive days and dividing the difference by the closing price of the previous day. For the first day, we are taking the differnece between the closing and opening price of that day and dividing it by the opening price of that day. 

```{r}
allfeatures_df["Price_Return"] <- NA
allfeatures_df$Price_Return[1] = (allfeatures_df$Close[1] - allfeatures_df$Open[1])/allfeatures_df$Open[1] ;
for (i in 2:310){
allfeatures_df$Price_Return[i] = (allfeatures_df$Close[i] -  allfeatures_df$Close[i-1])/allfeatures_df$Close[i-1]; 
  i = i + 1;
}
```

### We are adding a new feature to the features dataframe i.e. Percentage in Price change. The change is calculated by taking the difference of the closing price and opening price of the particular day and dividing it by the opening price of that day and multiplying it with 100. 

```{r}
allfeatures_df["Percentage_Price_Change"] <- NA
for (i in 1:310){
allfeatures_df$Percentage_Price_Change[i] =((allfeatures_df$Close[i] -  allfeatures_df$Open[i])/allfeatures_df$Open[i])*100; 
  i = i + 1;
}
head(allfeatures_df)
```


### For creating a linear model we need to create a vector that consist of the column names that will be the features that we will be using for generating the model. The co-efficients for the x-values are shown. For creating a linear regression model, lm() function is used. It accepts the formula and the data as arguement.The features that we are using are No_of_Transaction, Percentage in price change of a day, Hihgest and the lowest price of day.

```{r}

input <- allfeatures_df[,c("High","Low","No_Of_Transaction","Price_Return","Percentage_Price_Change")]
model <- lm(Price_Return ~ No_Of_Transaction + Percentage_Price_Change + High + Low, data=input)

print(model)

```

### The summary of the generated model is displayed here along with the adjusted R-squared value.

```{r}
summary(model)
```

#Conclusion

<font size ="4">R-squared explains the degree to which the input variables explain the variation of the output / predicted variable. Higher the R squared, the more variation is explained by the input variables and hence better is the model.However, the problem with R-squared is that it will either stay the same or increase with addition of more variables, even if they do not have any relationship with the output variables. This is where “Adjusted R square” comes to help. Adjusted R-square penalizes for adding variables which do not improve your existing model. Hence,for building a Linear regression model on multiple variables, it is always suggested to use Adjusted R-squared to judge goodness of model.From the above mentioned features, the Mutiple Linear Regression Model gave us the value of Adjusted R-squared as 0.8837 which indicates that the model is closely fitted to the data.</font>

#References

https://etherscan.io/token/0x86fa049857e0209aa7d9e616f7eb3b3b78ecfdb0

https://cran.r-project.org/web/packages/ggpubr/index.html

https://cran.r-project.org/web/packages/fitdistrplus/index.html

https://www.datasciencecentral.com/profiles/blogs/how-to-forecast-using-regression-analysis-in-r

http://www.stat.columbia.edu/~martin/W2024/R6.pdf

https://www.r-bloggers.com/r-tutorial-series-simple-linear-regression/













